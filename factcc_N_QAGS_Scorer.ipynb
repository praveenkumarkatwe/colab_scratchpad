{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1sWOdrpqkAQ0dxYchv4w6wF90M-yqo62O",
      "authorship_tag": "ABX9TyOwOZIo/5uBeZ8mOvm0sLD7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praveenkumarkatwe/colab_scratchpad/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "XSbAcNtk4eMC",
        "outputId": "46822428-8107-4421-e338-bf7bcbb6dbfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-1374580336.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1374580336.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Compute FactCC and QAGS scores for summarization outputs.\n",
        "\n",
        "This script extends the earlier ``compute_factcc_feqa_scores.py`` by replacing\n",
        "the FEQA/QuestEval component with **QAGS** (Question Answering and\n",
        "Generation for Summarization).  QAGS is a question‑answering based metric\n",
        "introduced by Wang et al. (2020) that assesses factual consistency by\n",
        "comparing answers to automatically generated questions when the source\n",
        "article and the model summary are used as context【596674913693464†L117-L143】.  The\n",
        "`factsumm` library provides a convenient implementation of QAGS via the\n",
        "``FactSumm`` class.  For each pair of source and summary, the method\n",
        "``extract_qas`` generates questions from the summary, answers them using\n",
        "both the source and the summary, and returns the average F1 overlap of\n",
        "answers (the QAGS score)【456578228132877†L305-L361】.\n",
        "\n",
        "The script reads a JSON file containing records with at least the following\n",
        "fields:\n",
        "\n",
        "* ``text`` – the source document or passage to be summarized.\n",
        "* ``reference`` – the reference (gold) summary (optional but retained for\n",
        "  completeness).\n",
        "* ``generated_before`` – the system summary before fine‑tuning.\n",
        "* ``generated_after`` – the system summary after fine‑tuning.\n",
        "\n",
        "It computes two factuality scores for each of the ``generated_before`` and\n",
        "``generated_after`` summaries:\n",
        "\n",
        "1. **FactCC** – a classifier‑based probability that the summary is\n",
        "   factually consistent with the source【484315385132402†L6-L23】.  We load the\n",
        "   ``manueldeprada/FactCC`` model via Hugging Face ``transformers`` and\n",
        "   report the softmax probability of the ``factual`` class.\n",
        "2. **QAGS** – a QA‑based faithfulness score computed via ``FactSumm``\n",
        "   in ``factsumm``.  ``extract_qas`` returns a float between 0 and 1\n",
        "   representing the average F1 overlap of answers to questions posed on\n",
        "   the source and summary【456578228132877†L305-L361】.\n",
        "\n",
        "The resulting augmented records include four new keys per record:\n",
        "\n",
        "* ``factcc_before`` – FactCC score for the pre‑fine‑tuning summary.\n",
        "* ``qags_before`` – QAGS score for the pre‑fine‑tuning summary.\n",
        "* ``factcc_after`` – FactCC score for the post‑fine‑tuning summary.\n",
        "* ``qags_after`` – QAGS score for the post‑fine‑tuning summary.\n",
        "\n",
        "### Usage\n",
        "\n",
        "Install dependencies:\n",
        "\n",
        "```bash\n",
        "pip install transformers torch factsumm\n",
        "```\n",
        "\n",
        "Then run the script over your JSON file:\n",
        "\n",
        "```bash\n",
        "python compute_factcc_qags_scores.py --input_path input.json --output_path output.json\n",
        "```\n",
        "\n",
        "You can optionally add ``--use_cuda`` to place the FactCC model on GPU and\n",
        "use GPU for QAGS if available.  Note that ``factsumm`` will automatically\n",
        "download several pre‑trained models (NER, relation extraction, question\n",
        "generation, question answering) the first time it is used【703489966589102†L46-L116】.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Import FactSumm for QAGS computation.  We wrap this in a try/except\n",
        "# to provide a helpful error message if the user has not installed\n",
        "# ``factsumm``.  FactSumm encapsulates all the components needed to\n",
        "# generate and answer questions and compute the QAGS score【456578228132877†L305-L361】.\n",
        "try:\n",
        "    from factsumm import FactSumm\n",
        "except ImportError as exc:\n",
        "    raise ImportError(\n",
        "        \"factsumm is required for QAGS scoring but is not installed. \"\n",
        "        \"Please install it with `pip install factsumm` and ensure your \"\n",
        "        \"environment has access to the internet to download the underlying models.\"\n",
        "    ) from exc\n",
        "\n",
        "\n",
        "def load_models(\n",
        "    model_name: str = \"manueldeprada/FactCC\",\n",
        "    use_cuda: bool = False,\n",
        ") -> tuple:\n",
        "    \"\"\"Load the FactCC classifier and the FactSumm QAGS scorer.\n",
        "\n",
        "    Args:\n",
        "        model_name: Hugging Face identifier for the FactCC model.\n",
        "        use_cuda: Whether to load models on GPU if available.\n",
        "\n",
        "    Returns:\n",
        "        A tuple ``(tokenizer, classifier, factsumm)``.\n",
        "    \"\"\"\n",
        "    # Load FactCC tokenizer and classifier\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    classifier = BertForSequenceClassification.from_pretrained(model_name)\n",
        "    classifier.eval()\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "        classifier.to(\"cuda\")\n",
        "    # Instantiate FactSumm.  The constructor sets up configuration for\n",
        "    # named entity recognition, relation extraction, question generation\n",
        "    # and question answering using default models.  The heavy models are\n",
        "    # loaded lazily on first use.\n",
        "    factsumm = FactSumm()\n",
        "    return tokenizer, classifier, factsumm\n",
        "\n",
        "\n",
        "def compute_factcc_score(\n",
        "    classifier: BertForSequenceClassification,\n",
        "    tokenizer: BertTokenizer,\n",
        "    source: str,\n",
        "    summary: str,\n",
        ") -> float:\n",
        "    \"\"\"Compute the FactCC probability that ``summary`` is factual for ``source``.\n",
        "\n",
        "    The classifier predicts two logits, corresponding to ``non‑factual`` and\n",
        "    ``factual`` classes.  After applying softmax we return the probability of\n",
        "    the second class (index 1).  See the FactCC paper for details【484315385132402†L6-L23】.\n",
        "\n",
        "    Args:\n",
        "        classifier: Pre‑trained FactCC classifier.\n",
        "        tokenizer: Tokenizer associated with the FactCC model.\n",
        "        source: Original source document.\n",
        "        summary: Candidate summary.\n",
        "\n",
        "    Returns:\n",
        "        A float in [0, 1] representing the FactCC factuality score.\n",
        "    \"\"\"\n",
        "    # Tokenize the source and summary.  The sequence is truncated at 512\n",
        "    # tokens (the BERT maximum) and padded to ensure fixed length.\n",
        "    inputs = tokenizer(\n",
        "        source,\n",
        "        summary,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        logits = classifier(**inputs).logits\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    factual_prob = probs[0, 1].item()\n",
        "    return float(factual_prob)\n",
        "\n",
        "\n",
        "def compute_qags_score(\n",
        "    factsumm: FactSumm,\n",
        "    source: str,\n",
        "    summary: str,\n",
        "    use_cuda: bool = False,\n",
        ") -> float:\n",
        "    \"\"\"Compute the QAGS score for a (source, summary) pair using FactSumm.\n",
        "\n",
        "    QAGS compares answers to questions generated from the summary when\n",
        "    conditioned on the source versus conditioned on the summary.  The\n",
        "    underlying implementation generates questions with a T5 model and\n",
        "    answers them with a BERT‑style QA model【456578228132877†L305-L361】.\n",
        "\n",
        "    Args:\n",
        "        factsumm: Instantiated ``FactSumm`` object.\n",
        "        source: Original source document.\n",
        "        summary: Candidate summary.\n",
        "        use_cuda: Whether to run QAGS on GPU (if available).  FactSumm\n",
        "            internally takes a ``device`` string: ``\"cuda\"`` or ``\"cpu\"``.\n",
        "\n",
        "    Returns:\n",
        "        A float QAGS score between 0 and 1.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\"\n",
        "    # ``extract_qas`` returns the QAGS score directly.  We pass\n",
        "    # ``verbose=False`` to suppress intermediate prints.\n",
        "    score = factsumm.extract_qas(\n",
        "        source,\n",
        "        summary,\n",
        "        source_ents=None,\n",
        "        summary_ents=None,\n",
        "        verbose=False,\n",
        "        device=device,\n",
        "    )\n",
        "    # ``extract_qas`` logs the QAGS score, but also returns it.  We cast to\n",
        "    # float for JSON serialization.\n",
        "    return float(score)\n",
        "\n",
        "\n",
        "def process_json(\n",
        "    records: List[Dict[str, Any]],\n",
        "    tokenizer: BertTokenizer,\n",
        "    classifier: BertForSequenceClassification,\n",
        "    factsumm: FactSumm,\n",
        "    use_cuda: bool = False,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"Augment each record with FactCC and QAGS scores.\n",
        "\n",
        "    Args:\n",
        "        records: List of input records loaded from JSON.\n",
        "        tokenizer: FactCC tokenizer.\n",
        "        classifier: FactCC classifier.\n",
        "        factsumm: FactSumm object for QAGS computation.\n",
        "        use_cuda: Whether to use GPU.\n",
        "\n",
        "    Returns:\n",
        "        The list of records with new score fields.\n",
        "    \"\"\"\n",
        "    for idx, item in enumerate(records):\n",
        "        source = item.get(\"text\") or item.get(\"source\")\n",
        "        reference = item.get(\"reference\") or item.get(\"reference_summary\")\n",
        "        before = (\n",
        "            item.get(\"generated_before\")\n",
        "            or item.get(\"summary_before\")\n",
        "            or item.get(\"generated_summary\")\n",
        "        )\n",
        "        after = (\n",
        "            item.get(\"generated_after\")\n",
        "            or item.get(\"summary_after\")\n",
        "            or item.get(\"generated_finetuned\")\n",
        "        )\n",
        "        if source is None or before is None or after is None:\n",
        "            raise KeyError(\n",
        "                f\"Record {idx} is missing required keys: 'text', 'generated_before', 'generated_after'.\"\n",
        "            )\n",
        "        # Compute FactCC scores for before and after summaries\n",
        "        item[\"factcc_before\"] = compute_factcc_score(classifier, tokenizer, source, before)\n",
        "        item[\"factcc_after\"] = compute_factcc_score(classifier, tokenizer, source, after)\n",
        "        # Compute QAGS scores using FactSumm.  We ignore the reference here\n",
        "        # since QAGS only uses source and summary.\n",
        "        try:\n",
        "            qags_before = compute_qags_score(factsumm, source, before, use_cuda)\n",
        "        except Exception:\n",
        "            qags_before = 0.0\n",
        "        try:\n",
        "            qags_after = compute_qags_score(factsumm, source, after, use_cuda)\n",
        "        except Exception:\n",
        "            qags_after = 0.0\n",
        "        item[\"qags_before\"] = qags_before\n",
        "        item[\"qags_after\"] = qags_after\n",
        "    return records\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=(\n",
        "            \"Compute FactCC and QAGS scores for summaries in a JSON file.\"\n",
        "        )\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--input_path\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Path to the input JSON file.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_path\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=(\n",
        "            \"Path to save the scored JSON.  Defaults to the input filename with \"\n",
        "            \"'_scored' appended before the extension.\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--use_cuda\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to run FactCC and QAGS on CUDA if available.\",\n",
        "    )\n",
        "\n",
        "    # Determine output path\n",
        "\n",
        "    input_path=\"output.json\"\n",
        "    output_path=\"output_scored.json\"\n",
        "    use_cuda=False\n",
        "    if output_path is None:\n",
        "        base, ext = os.path.splitext(input_path)\n",
        "        output_path = f\"{base}_scored{ext}\"\n",
        "    # Load input JSON\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as fin:\n",
        "        data = json.load(fin)\n",
        "    if not isinstance(data, list):\n",
        "        raise ValueError(\"Input JSON must be a list of records.\")\n",
        "    # Load models\n",
        "    tokenizer, classifier, factsumm = load_models(use_cuda=args.use_cuda)\n",
        "    # Process and score\n",
        "    scored_data = process_json(\n",
        "        data,\n",
        "        tokenizer,\n",
        "        classifier,\n",
        "        factsumm,\n",
        "        use_cuda=use_cuda,\n",
        "    )\n",
        "    # Write output\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "        json.dump(scored_data, fout, indent=2, ensure_ascii=False)\n",
        "    print(f\"Wrote scored records to {args.output_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "KFE9AsPz_nHf",
        "outputId": "fde35a49-3003-498f-fe41-8a46c5669091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'output.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4222986537.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4222986537.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{base}_scored{ext}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;31m# Load input JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "compute_factcc_qags_scores.py\n",
        "\n",
        "Add FactCC-like and QAGS-like factual consistency scores to a before/after\n",
        "summarization dataset.\n",
        "\n",
        "Input JSON format (list of dicts). The script is flexible and recognizes\n",
        "multiple key names. For your file, these are the important ones:\n",
        "\n",
        "- Source text:\n",
        "    - preferred: \"input\"\n",
        "    - also accepted: \"text\", \"source\"\n",
        "\n",
        "- Reference summary (optional; unused by default for QAGS but kept if needed):\n",
        "    - \"reference\", \"reference_summary\"\n",
        "\n",
        "- BEFORE summary:\n",
        "    - preferred: \"generatedsummary_before\"\n",
        "    - also accepted: \"generated_before\", \"summary_before\", \"generated_summary\"\n",
        "\n",
        "- AFTER summary:\n",
        "    - preferred: \"generatedsummary_after\"\n",
        "    - also accepted: \"generated_after\", \"summary_after\", \"generated_finetuned\"\n",
        "\n",
        "Outputs (per record, appended):\n",
        "    - \"factcc_before\", \"factcc_after\"      -> probability that summary is consistent\n",
        "    - \"qags_before\",   \"qags_after\"        -> average token-F1 between QA answers\n",
        "                                              from source vs. summary, across\n",
        "                                              auto-generated questions.\n",
        "\n",
        "USAGE:\n",
        "    pip install -q transformers torch accelerate\n",
        "    # for QAGS-like (QG + QA):\n",
        "    pip install -q sentencepiece\n",
        "\n",
        "    python compute_factcc_qags_scores.py \\\n",
        "        --input_path /path/to/before_after.json \\\n",
        "        --output_path /path/to/before_after_scored.json\n",
        "\n",
        "You can change model names:\n",
        "    --factcc_model tals/albert-base-v2-factcc\n",
        "    --qg_model valhalla/t5-small-qg-hl\n",
        "    --qa_model deepset/roberta-base-squad2\n",
        "\n",
        "NOTES:\n",
        "- \"FactCC-like\": We use a binary/textual entailment-style classifier that accepts\n",
        "  (source, summary) as a sequence pair and returns a consistency probability.\n",
        "  If the chosen model exposes labels, we map to 'consistent' as best as possible.\n",
        "\n",
        "- \"QAGS-like\": We generate questions from the summary, then answer each question\n",
        "  twice: once from the SOURCE ('answer_src') and once from the SUMMARY\n",
        "  ('answer_sum'). We compute token-level F1(answer_src, answer_sum) over the\n",
        "  generated questions and average. Higher is better.\n",
        "\n",
        "- Truncation: Long source texts are truncated to model max length; you can\n",
        "  adjust with --max_source_tokens and --max_summary_tokens.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "\n",
        "def pick_first(*vals):\n",
        "    \"\"\"Return the first non-empty value from provided candidates (strings).\"\"\"\n",
        "    for v in vals:\n",
        "        if v is None:\n",
        "            continue\n",
        "        if isinstance(v, str) and v.strip() == \"\":\n",
        "            continue\n",
        "        return v\n",
        "    return None\n",
        "\n",
        "\n",
        "def tokenize_pair(tokenizer, a: str, b: str, max_a: int, max_b: int):\n",
        "    \"\"\"Tokenize a pair (a=source, b=summary) with independent truncation budgets.\"\"\"\n",
        "    # Strategy: truncate each string individually, then let tokenizer pair them.\n",
        "    # We do a rough truncation by tokens using tokenizer.encode with truncation=True.\n",
        "    a_ids = tokenizer.encode(a, add_special_tokens=False, truncation=True, max_length=max_a)\n",
        "    b_ids = tokenizer.encode(b, add_special_tokens=False, truncation=True, max_length=max_b)\n",
        "\n",
        "    # Build pair with special tokens\n",
        "    pair = tokenizer.prepare_for_model(\n",
        "        a_ids,\n",
        "        b_ids,\n",
        "        truncation=True,\n",
        "        max_length=max_a + max_b + 3,  # [CLS], [SEP], [SEP] (model-dependent)\n",
        "        return_tensors=\"pt\",\n",
        "        add_special_tokens=True,\n",
        "    )\n",
        "    return pair\n",
        "\n",
        "\n",
        "def softmax(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
        "    return torch.nn.functional.softmax(x, dim=dim)\n",
        "\n",
        "\n",
        "def sanitize(s: Optional[str]) -> str:\n",
        "    return s if (isinstance(s, str) and len(s) > 0) else \"\"\n",
        "\n",
        "\n",
        "def simple_token_f1(a: str, b: str) -> float:\n",
        "    \"\"\"Token-level F1 (space-split; case-insensitive; strips punctuation lightly).\"\"\"\n",
        "    import re\n",
        "    def norm(t: str) -> List[str]:\n",
        "        t = t.lower()\n",
        "        t = re.sub(r\"[^a-z0-9\\s]\", \" \", t)\n",
        "        toks = [x for x in t.split() if x]\n",
        "        return toks\n",
        "\n",
        "    A, B = norm(a), norm(b)\n",
        "    if not A and not B:\n",
        "        return 1.0\n",
        "    if not A or not B:\n",
        "        return 0.0\n",
        "    # overlap\n",
        "    from collections import Counter\n",
        "    cA, cB = Counter(A), Counter(B)\n",
        "    overlap = sum((cA & cB).values())\n",
        "    if overlap == 0:\n",
        "        return 0.0\n",
        "    precision = overlap / max(1, sum(cA.values()))\n",
        "    recall    = overlap / max(1, sum(cB.values()))\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# FactCC-like scoring\n",
        "# ----------------------------\n",
        "\n",
        "class FactCCScorer:\n",
        "    \"\"\"\n",
        "    Wraps a sequence classifier to output P(consistent | source, summary).\n",
        "\n",
        "    We attempt to map label indices to {consistent, inconsistent} by inspecting\n",
        "    model.config.id2label. If unknown, we assume the higher logit corresponds to 'consistent'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str, device: str = \"auto\"):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        if device == \"auto\":\n",
        "            self.device = 0 if torch.cuda.is_available() else -1\n",
        "        elif device == \"cpu\":\n",
        "            self.device = -1\n",
        "        else:\n",
        "            # assume it's an integer CUDA device id\n",
        "            try:\n",
        "                self.device = int(device)\n",
        "            except Exception:\n",
        "                self.device = -1\n",
        "\n",
        "        if self.device >= 0:\n",
        "            self.model.to(f\"cuda:{self.device}\")\n",
        "        self.model.eval()\n",
        "\n",
        "        # try to infer label mapping\n",
        "        self.consistent_label_idx = None\n",
        "        id2label = getattr(self.model.config, \"id2label\", None)\n",
        "        if isinstance(id2label, dict) and len(id2label) == self.model.config.num_labels:\n",
        "            for k, v in id2label.items():\n",
        "                name = str(v).lower()\n",
        "                if any(t in name for t in [\"entail\", \"support\", \"consistent\", \"true\", \"label_1\"]):\n",
        "                    self.consistent_label_idx = int(k)\n",
        "                    break\n",
        "            # fallback: if labels look like {0: 'LABEL_0', 1: 'LABEL_1'}\n",
        "            if self.consistent_label_idx is None and 1 in id2label:\n",
        "                self.consistent_label_idx = 1  # assume positive=1\n",
        "        if self.consistent_label_idx is None:\n",
        "            # unknown mapping; will take max logit as 'consistent'\n",
        "            self.consistent_label_idx = -1\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def score(self, source: str, summary: str,\n",
        "              max_source_tokens: int = 512, max_summary_tokens: int = 256) -> float:\n",
        "        source = sanitize(source)\n",
        "        summary = sanitize(summary)\n",
        "        if not source or not summary:\n",
        "            return 0.0\n",
        "\n",
        "        pair = tokenize_pair(self.tokenizer, source, summary, max_source_tokens, max_summary_tokens)\n",
        "        if self.device >= 0:\n",
        "            pair = {k: v.to(f\"cuda:{self.device}\") for k, v in pair.items()}\n",
        "        logits = self.model(**pair).logits  # [1, num_labels]\n",
        "        probs = softmax(logits, dim=-1).squeeze(0)\n",
        "\n",
        "        if self.consistent_label_idx == -1:\n",
        "            # take max as 'consistent'\n",
        "            return float(torch.max(probs).item())\n",
        "        else:\n",
        "            return float(probs[self.consistent_label_idx].item())\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# QAGS-like scoring\n",
        "# ----------------------------\n",
        "\n",
        "class QAGSLite:\n",
        "    \"\"\"\n",
        "    QAGS-like scorer: generate questions from SUMMARY, then answer the questions\n",
        "    twice (from SOURCE and from SUMMARY). Compute token-level F1 between the\n",
        "    two answers and average.\n",
        "\n",
        "    This is a lightweight approximation of QAGS:\n",
        "    - Question Generation: a seq2seq T5 model fine-tuned for QG with highlighting.\n",
        "    - QA: extractive QA model (SQuAD2-style).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 qg_model: str = \"valhalla/t5-small-qg-hl\",\n",
        "                 qa_model: str = \"deepset/roberta-base-squad2\",\n",
        "                 device: str = \"auto\"):\n",
        "        # device handling for pipelines:\n",
        "        if device == \"auto\":\n",
        "            self.device = 0 if torch.cuda.is_available() else -1\n",
        "        elif device == \"cpu\":\n",
        "            self.device = -1\n",
        "        else:\n",
        "            try:\n",
        "                self.device = int(device)\n",
        "            except Exception:\n",
        "                self.device = -1\n",
        "\n",
        "        # QG pipeline: we will manually craft highlights and feed through text2text\n",
        "        self.qg_tokenizer = AutoTokenizer.from_pretrained(qg_model, use_fast=True)\n",
        "        self.qg_model = AutoModelForSeq2SeqLM.from_pretrained(qg_model)\n",
        "        if self.device >= 0:\n",
        "            self.qg_model.to(f\"cuda:{self.device}\")\n",
        "        self.qg_model.eval()\n",
        "\n",
        "        # QA pipeline\n",
        "        self.qa_pipe = pipeline(\n",
        "            task=\"question-answering\",\n",
        "            model=qa_model,\n",
        "            tokenizer=qa_model,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _gen_questions(self, summary: str, max_questions: int = 5,\n",
        "                       max_input_len: int = 384, max_out_len: int = 32) -> List[str]:\n",
        "        \"\"\"\n",
        "        Very simple QG using \"highlight\" trick:\n",
        "          input: \"generate questions: <hl> sentence <hl> rest\"\n",
        "        We'll split the summary into sentences and highlight each one to prompt QG.\n",
        "        \"\"\"\n",
        "        import re\n",
        "        sents = [s.strip() for s in re.split(r'(?<=[\\.\\?\\!])\\s+', summary) if s.strip()]\n",
        "        questions = []\n",
        "        for sent in sents:\n",
        "            prompt = f\"generate questions: <hl> {sent} <hl> {summary}\"\n",
        "            enc = self.qg_tokenizer(\n",
        "                prompt,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                max_length=max_input_len\n",
        "            )\n",
        "            if self.device >= 0:\n",
        "                enc = {k: v.to(f\"cuda:{self.device}\") for k, v in enc.items()}\n",
        "            out = self.qg_model.generate(\n",
        "                **enc,\n",
        "                num_beams=4,\n",
        "                max_length=max_out_len,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            text = self.qg_tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
        "            # Models often emit multiple questions separated by \"?\" or \"<sep>\"\n",
        "            parts = re.split(r'\\?|\\<sep\\>', text)\n",
        "            for p in parts:\n",
        "                q = p.strip()\n",
        "                if q:\n",
        "                    if not q.endswith(\"?\"):\n",
        "                        q += \"?\"\n",
        "                    questions.append(q)\n",
        "            if len(questions) >= max_questions:\n",
        "                break\n",
        "        # de-dup and trim\n",
        "        dedup = []\n",
        "        seen = set()\n",
        "        for q in questions:\n",
        "            qn = q.lower()\n",
        "            if qn not in seen:\n",
        "                seen.add(qn)\n",
        "                dedup.append(q)\n",
        "        return dedup[:max_questions]\n",
        "\n",
        "    def _answer(self, question: str, context: str) -> str:\n",
        "        context = sanitize(context)\n",
        "        if not question or not context:\n",
        "            return \"\"\n",
        "        try:\n",
        "            out = self.qa_pipe({\"question\": question, \"context\": context})\n",
        "            if isinstance(out, list) and out:\n",
        "                out = out[0]\n",
        "            ans = out.get(\"answer\", \"\") if isinstance(out, dict) else \"\"\n",
        "            return ans.strip()\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "\n",
        "    def score(self, source: str, summary: str,\n",
        "              max_questions: int = 5) -> float:\n",
        "        summary = sanitize(summary)\n",
        "        source  = sanitize(source)\n",
        "        if not summary or not source:\n",
        "            return 0.0\n",
        "\n",
        "        questions = self._gen_questions(summary, max_questions=max_questions)\n",
        "        if not questions:\n",
        "            return 0.0\n",
        "\n",
        "        f1s = []\n",
        "        for q in questions:\n",
        "            ans_src = self._answer(q, source)\n",
        "            ans_sum = self._answer(q, summary)\n",
        "            f1s.append(simple_token_f1(ans_src, ans_sum))\n",
        "        if not f1s:\n",
        "            return 0.0\n",
        "        return float(sum(f1s) / len(f1s))\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Processing logic\n",
        "# ----------------------------\n",
        "\n",
        "def process_json(records: List[Dict[str, Any]],\n",
        "                 factcc: FactCCScorer,\n",
        "                 qags: QAGSLite,\n",
        "                 max_source_tokens: int,\n",
        "                 max_summary_tokens: int,\n",
        "                 qags_max_questions: int) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Augment each record with FactCC and QAGS-like scores.\n",
        "    Handles multiple key variants and raises a clear error if a required field is missing.\n",
        "    \"\"\"\n",
        "    for idx, item in enumerate(records):\n",
        "        source = pick_first(item.get(\"text\"),\n",
        "                           item.get(\"source\"),\n",
        "                           item.get(\"input\"))\n",
        "        reference = pick_first(item.get(\"reference\"),\n",
        "                               item.get(\"reference_summary\"))\n",
        "\n",
        "        before = pick_first(item.get(\"generated_before\"),\n",
        "                           item.get(\"summary_before\"),\n",
        "                           item.get(\"generated_summary\"),\n",
        "                           item.get(\"generatedsummary_before\"))\n",
        "\n",
        "        after  = pick_first(item.get(\"generated_after\"),\n",
        "                           item.get(\"summary_after\"),\n",
        "                           item.get(\"generated_finetuned\"),\n",
        "                           item.get(\"generatedsummary_after\"))\n",
        "\n",
        "        if source is None or before is None or after is None:\n",
        "            raise KeyError(\n",
        "                f\"Record {idx} is missing required keys:\\n\"\n",
        "                f\"  source in ['text','source','input'] -> {bool(source)}\\n\"\n",
        "                f\"  before in ['generated_before','summary_before','generated_summary','generatedsummary_before'] -> {bool(before)}\\n\"\n",
        "                f\"  after  in ['generated_after','summary_after','generated_finetuned','generatedsummary_after'] -> {bool(after)}\"\n",
        "            )\n",
        "\n",
        "        # FactCC-like\n",
        "        try:\n",
        "            item[\"factcc_before\"] = factcc.score(source, before,\n",
        "                                                 max_source_tokens=max_source_tokens,\n",
        "                                                 max_summary_tokens=max_summary_tokens)\n",
        "        except Exception:\n",
        "            item[\"factcc_before\"] = 0.0\n",
        "\n",
        "        try:\n",
        "            item[\"factcc_after\"] = factcc.score(source, after,\n",
        "                                                max_source_tokens=max_source_tokens,\n",
        "                                                max_summary_tokens=max_summary_tokens)\n",
        "        except Exception:\n",
        "            item[\"factcc_after\"] = 0.0\n",
        "\n",
        "        # QAGS-like\n",
        "        try:\n",
        "            item[\"qags_before\"] = qags.score(source, before,\n",
        "                                             max_questions=qags_max_questions)\n",
        "        except Exception:\n",
        "            item[\"qags_before\"] = 0.0\n",
        "\n",
        "        try:\n",
        "            item[\"qags_after\"] = qags.score(source, after,\n",
        "                                            max_questions=qags_max_questions)\n",
        "        except Exception:\n",
        "            item[\"qags_after\"] = 0.0\n",
        "\n",
        "        # Keep reference around if you want to do reference-aware analysis later\n",
        "        if reference is not None and \"reference\" not in item:\n",
        "            item[\"reference\"] = reference\n",
        "\n",
        "    return records\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# CLI\n",
        "# ----------------------------\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser(description=\"Compute FactCC-like and QAGS-like scores for before/after summaries.\")\n",
        "    p.add_argument(\"--input_path\", required=False, help=\"Path to input JSON (list of dicts).\")\n",
        "    p.add_argument(\"--output_path\", required=False, help=\"Where to write augmented JSON.\")\n",
        "    p.add_argument(\"--factcc_model\", default=\"tals/albert-base-v2-factcc\",\n",
        "                   help=\"HF model for pairwise consistency classification.\")\n",
        "    p.add_argument(\"--qg_model\", default=\"valhalla/t5-small-qg-hl\",\n",
        "                   help=\"HF seq2seq model for question generation.\")\n",
        "    p.add_argument(\"--qa_model\", default=\"deepset/roberta-base-squad2\",\n",
        "                   help=\"HF extractive QA model.\")\n",
        "    p.add_argument(\"--device\", default=\"auto\",\n",
        "                   help=\"Device for models: 'auto' | 'cpu' | CUDA index (e.g., '0').\")\n",
        "    p.add_argument(\"--max_source_tokens\", type=int, default=448,\n",
        "                   help=\"Max tokens reserved for source in the FactCC pair.\")\n",
        "    p.add_argument(\"--max_summary_tokens\", type=int, default=192,\n",
        "                   help=\"Max tokens reserved for summary in the FactCC pair.\")\n",
        "    p.add_argument(\"--qags_max_questions\", type=int, default=5,\n",
        "                   help=\"Maximum questions to generate per summary for QAGS-like scoring.\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    args.input_path=\"output.json\"\n",
        "    args.output_path=\"output_scored.json\"\n",
        "    args.use_cuda=False\n",
        "    # Load input\n",
        "    with open(args.input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    if not isinstance(data, list):\n",
        "        raise ValueError(\"Input JSON must be a list of objects.\")\n",
        "\n",
        "    # Init scorers\n",
        "    factcc = FactCCScorer(model_name=args.factcc_model, device=args.device)\n",
        "    qags = QAGSLite(qg_model=args.qg_model, qa_model=args.qa_model, device=args.device)\n",
        "\n",
        "    # Process\n",
        "    data = process_json(\n",
        "        records=data,\n",
        "        factcc=factcc,\n",
        "        qags=qags,\n",
        "        max_source_tokens=args.max_source_tokens,\n",
        "        max_summary_tokens=args.max_summary_tokens,\n",
        "        qags_max_questions=args.qags_max_questions,\n",
        "    )\n",
        "\n",
        "    # Save\n",
        "    with open(args.output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"✅ Wrote augmented JSON to: {args.output_path}\")\n",
        "    print(\"Keys added per record: factcc_before, factcc_after, qags_before, qags_after\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "0sumPu4VEE7w",
        "outputId": "7a499b38-deb9-43a8-f517-25d4f7a97945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--input_path INPUT_PATH]\n",
            "                                [--output_path OUTPUT_PATH]\n",
            "                                [--factcc_model FACTCC_MODEL]\n",
            "                                [--qg_model QG_MODEL] [--qa_model QA_MODEL]\n",
            "                                [--device DEVICE]\n",
            "                                [--max_source_tokens MAX_SOURCE_TOKENS]\n",
            "                                [--max_summary_tokens MAX_SUMMARY_TOKENS]\n",
            "                                [--qags_max_questions QAGS_MAX_QUESTIONS]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-422a61aa-88fc-440f-9518-b2976dfb3cbe.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 🧠 Define scorers and helpers (run once)\n",
        "\n",
        "import json, re, math, os, sys\n",
        "from typing import Any, Dict, List, Optional\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "def pick_first(*vals):\n",
        "    for v in vals:\n",
        "        if v is None:\n",
        "            continue\n",
        "        if isinstance(v, str) and v.strip() == \"\":\n",
        "            continue\n",
        "        return v\n",
        "    return None\n",
        "\n",
        "def sanitize(s: Optional[str]) -> str:\n",
        "    return s if (isinstance(s, str) and len(s) > 0) else \"\"\n",
        "\n",
        "def softmax(x: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
        "    return torch.nn.functional.softmax(x, dim=dim)\n",
        "\n",
        "def tokenize_pair(tokenizer, a: str, b: str, max_a: int, max_b: int):\n",
        "    a_ids = tokenizer.encode(a, add_special_tokens=False, truncation=True, max_length=max_a)\n",
        "    b_ids = tokenizer.encode(b, add_special_tokens=False, truncation=True, max_length=max_b)\n",
        "    pair = tokenizer.prepare_for_model(\n",
        "        a_ids, b_ids,\n",
        "        truncation=True,\n",
        "        max_length=max_a + max_b + 3,\n",
        "        return_tensors=\"pt\",\n",
        "        add_special_tokens=True,\n",
        "    )\n",
        "    return pair\n",
        "\n",
        "def simple_token_f1(a: str, b: str) -> float:\n",
        "    def norm(t: str) -> List[str]:\n",
        "        t = t.lower()\n",
        "        t = re.sub(r\"[^a-z0-9\\s]\", \" \", t)\n",
        "        toks = [x for x in t.split() if x]\n",
        "        return toks\n",
        "    A, B = norm(a), norm(b)\n",
        "    if not A and not B:\n",
        "        return 1.0\n",
        "    if not A or not B:\n",
        "        return 0.0\n",
        "    from collections import Counter\n",
        "    cA, cB = Counter(A), Counter(B)\n",
        "    overlap = sum((cA & cB).values())\n",
        "    if overlap == 0:\n",
        "        return 0.0\n",
        "    precision = overlap / max(1, sum(cA.values()))\n",
        "    recall    = overlap / max(1, sum(cB.values()))\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "class FactCCScorer:\n",
        "    \"\"\"Binary (source, summary) consistency probability via sequence classifier.\"\"\"\n",
        "    def __init__(self, model_name: str, device: str = \"auto\"):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        if device == \"auto\":\n",
        "            self.device = 0 if torch.cuda.is_available() else -1\n",
        "        elif device == \"cpu\":\n",
        "            self.device = -1\n",
        "        else:\n",
        "            try:\n",
        "                self.device = int(device)\n",
        "            except Exception:\n",
        "                self.device = -1\n",
        "        if self.device >= 0:\n",
        "            self.model.to(f\"cuda:{self.device}\")\n",
        "        self.model.eval()\n",
        "\n",
        "        self.consistent_label_idx = None\n",
        "        id2label = getattr(self.model.config, \"id2label\", None)\n",
        "        if isinstance(id2label, dict) and len(id2label) == self.model.config.num_labels:\n",
        "            for k, v in id2label.items():\n",
        "                name = str(v).lower()\n",
        "                if any(t in name for t in [\"entail\", \"support\", \"consistent\", \"true\", \"label_1\"]):\n",
        "                    self.consistent_label_idx = int(k)\n",
        "                    break\n",
        "            if self.consistent_label_idx is None and 1 in id2label:\n",
        "                self.consistent_label_idx = 1\n",
        "        if self.consistent_label_idx is None:\n",
        "            self.consistent_label_idx = -1\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def score(self, source: str, summary: str,\n",
        "              max_source_tokens: int = 448, max_summary_tokens: int = 192) -> float:\n",
        "        source = sanitize(source)\n",
        "        summary = sanitize(summary)\n",
        "        if not source or not summary:\n",
        "            return 0.0\n",
        "        pair = tokenize_pair(self.tokenizer, source, summary, max_source_tokens, max_summary_tokens)\n",
        "        if self.device >= 0:\n",
        "            pair = {k: v.to(f\"cuda:{self.device}\") for k, v in pair.items()}\n",
        "        logits = self.model(**pair).logits\n",
        "        probs = softmax(logits, dim=-1).squeeze(0)\n",
        "        if self.consistent_label_idx == -1:\n",
        "            return float(torch.max(probs).item())\n",
        "        return float(probs[self.consistent_label_idx].item())\n",
        "\n",
        "class QAGSLite:\n",
        "    \"\"\"\n",
        "    Generate questions from SUMMARY and answer them from SOURCE and SUMMARY;\n",
        "    average token-F1 between the two answers (QAGS-style).\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 qg_model: str = \"valhalla/t5-small-qg-hl\",\n",
        "                 qa_model: str = \"deepset/roberta-base-squad2\",\n",
        "                 device: str = \"auto\"):\n",
        "        if device == \"auto\":\n",
        "            self.device = 0 if torch.cuda.is_available() else -1\n",
        "        elif device == \"cpu\":\n",
        "            self.device = -1\n",
        "        else:\n",
        "            try:\n",
        "                self.device = int(device)\n",
        "            except Exception:\n",
        "                self.device = int(device)\n",
        "        self.qg_tokenizer = AutoTokenizer.from_pretrained(qg_model, use_fast=True)\n",
        "        self.qg_model = AutoModelForSeq2SeqLM.from_pretrained(qg_model)\n",
        "        if self.device >= 0:\n",
        "            self.qg_model.to(f\"cuda:{self.device}\")\n",
        "        self.qg_model.eval()\n",
        "\n",
        "        self.qa_pipe = pipeline(\n",
        "            task=\"question-answering\",\n",
        "            model=qa_model,\n",
        "            tokenizer=qa_model,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _gen_questions(self, summary: str, max_questions: int = 5,\n",
        "                       max_input_len: int = 384, max_out_len: int = 32) -> List[str]:\n",
        "        sents = [s.strip() for s in re.split(r'(?<=[\\.\\?\\!])\\s+', summary) if s.strip()]\n",
        "        questions = []\n",
        "        for sent in sents:\n",
        "            prompt = f\"generate questions: <hl> {sent} <hl> {summary}\"\n",
        "            enc = self.qg_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_len)\n",
        "            if self.device >= 0:\n",
        "                enc = {k: v.to(f\"cuda:{self.device}\") for k, v in enc.items()}\n",
        "            out = self.qg_model.generate(**enc, num_beams=4, max_length=max_out_len, early_stopping=True)\n",
        "            text = self.qg_tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
        "            parts = re.split(r'\\?|\\<sep\\>', text)\n",
        "            for p in parts:\n",
        "                q = p.strip()\n",
        "                if q:\n",
        "                    if not q.endswith(\"?\"):\n",
        "                        q += \"?\"\n",
        "                    questions.append(q)\n",
        "            if len(questions) >= max_questions:\n",
        "                break\n",
        "        dedup, seen = [], set()\n",
        "        for q in questions:\n",
        "            qn = q.lower()\n",
        "            if qn not in seen:\n",
        "                seen.add(qn)\n",
        "                dedup.append(q)\n",
        "        return dedup[:max_questions]\n",
        "\n",
        "    def _answer(self, question: str, context: str) -> str:\n",
        "        context = sanitize(context)\n",
        "        if not question or not context:\n",
        "            return \"\"\n",
        "        try:\n",
        "            out = self.qa_pipe({\"question\": question, \"context\": context})\n",
        "            if isinstance(out, list) and out:\n",
        "                out = out[0]\n",
        "            return (out.get(\"answer\", \"\") if isinstance(out, dict) else \"\").strip()\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "\n",
        "    def score(self, source: str, summary: str, max_questions: int = 5) -> float:\n",
        "        summary = sanitize(summary); source = sanitize(source)\n",
        "        if not summary or not source:\n",
        "            return 0.0\n",
        "        questions = self._gen_questions(summary, max_questions=max_questions)\n",
        "        if not questions:\n",
        "            return 0.0\n",
        "        f1s = []\n",
        "        for q in questions:\n",
        "            ans_src = self._answer(q, source)\n",
        "            ans_sum = self._answer(q, summary)\n",
        "            f1s.append(simple_token_f1(ans_src, ans_sum))\n",
        "        return float(sum(f1s)/len(f1s)) if f1s else 0.0\n",
        "\n",
        "def process_json(records: List[Dict[str, Any]],\n",
        "                 factcc: FactCCScorer,\n",
        "                 qags: QAGSLite,\n",
        "                 max_source_tokens: int,\n",
        "                 max_summary_tokens: int,\n",
        "                 qags_max_questions: int) -> List[Dict[str, Any]]:\n",
        "    for idx, item in enumerate(records):\n",
        "        source = pick_first(item.get(\"text\"), item.get(\"source\"), item.get(\"input\"))\n",
        "        reference = pick_first(item.get(\"reference\"), item.get(\"reference_summary\"))\n",
        "        before = pick_first(item.get(\"generated_before\"),\n",
        "                            item.get(\"summary_before\"),\n",
        "                            item.get(\"generated_summary\"),\n",
        "                            item.get(\"generatedsummary_before\"),\n",
        "                            item.get(\"generatedsummary_before_EHI\")) # Added the new key here\n",
        "        after  = pick_first(item.get(\"generated_after\"),\n",
        "                            item.get(\"summary_after\"),\n",
        "                            item.get(\"generated_finetuned\"),\n",
        "                            item.get(\"generatedsummary_after\"),\n",
        "                            item.get(\"generatedsummary_after_EHI\")) # Added the new key here\n",
        "        if source is None or before is None or after is None:\n",
        "            raise KeyError(\n",
        "                f\"Record {idx} missing keys. \"\n",
        "                f\"source in ['text','source','input'] present? {bool(source)} | \"\n",
        "                f\"before in ['generated_before','summary_before','generated_summary','generatedsummary_before','generatedsummary_before_EHI'] present? {bool(before)} | \"\n",
        "                f\"after in ['generated_after','summary_after','generated_finetuned','generatedsummary_after','generatedsummary_after_EHI'] present? {bool(after)}\"\n",
        "            )\n",
        "        try:\n",
        "            item[\"factcc_before\"] = factcc.score(source, before, max_source_tokens, max_summary_tokens)\n",
        "        except Exception:\n",
        "            item[\"factcc_before\"] = 0.0\n",
        "        try:\n",
        "            item[\"factcc_after\"]  = factcc.score(source, after, max_source_tokens, max_summary_tokens)\n",
        "        except Exception:\n",
        "            item[\"factcc_after\"] = 0.0\n",
        "        try:\n",
        "            item[\"qags_before\"]   = qags.score(source, before, max_questions=qags_max_questions)\n",
        "        except Exception:\n",
        "            item[\"qags_before\"] = 0.0\n",
        "        try:\n",
        "            item[\"qags_after\"]    = qags.score(source, after, max_questions=qags_max_questions)\n",
        "        except Exception:\n",
        "            item[\"qags_after\"] = 0.0\n",
        "\n",
        "        if reference is not None and \"reference\" not in item:\n",
        "            item[\"reference\"] = reference\n",
        "    return records"
      ],
      "metadata": {
        "id": "gY8fU8OzFZGW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🚫 Force Transformers/HF Hub to run without any token\n",
        "import os\n",
        "# If your notebook (or Colab UI) injected a token, drop it:\n",
        "os.environ.pop(\"HF_TOKEN\", None)\n",
        "os.environ.pop(\"HUGGING_FACE_HUB_TOKEN\", None)\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"      # optional: quiet logs\n",
        "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"0\"          # make sure online downloads are OK\n",
        "print(\"HF tokens cleared; running without authentication.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m8-QyqfF_Le",
        "outputId": "20da954b-c3b7-4b1c-fd16-68dce597ca86"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF tokens cleared; running without authentication.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ▶️ Run scoring (upload JSON or point to Drive path)\n",
        "# @markdown Choose how to provide your input JSON:\n",
        "use_upload = True  # @param {type:\"boolean\"}\n",
        "# @markdown If you prefer to read from Google Drive, set `use_upload=False` and edit the `input_path` below.\n",
        "input_path = \"/content/drive/MyDrive/before_after_all_metrics_XSUM_mistral.json\"  # @param {type:\"string\"}\n",
        "\n",
        "# Model & runtime knobs\n",
        "factcc_model = \"tals/albert-base-v2-factcc\"  # @param {type:\"string\"}\n",
        "qg_model     = \"valhalla/t5-small-qg-hl\"     # @param {type:\"string\"}\n",
        "qa_model     = \"deepset/roberta-base-squad2\" # @param {type:\"string\"}\n",
        "device       = \"auto\"                        # @param [\"auto\",\"cpu\",\"0\",\"1\",\"2\",\"3\"]\n",
        "max_source_tokens   = 448                    # @param {type:\"integer\"}\n",
        "max_summary_tokens  = 192                    # @param {type:\"integer\"}\n",
        "qags_max_questions  = 5                      # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "factcc_model = \"manueldeprada/FactCC\"        # ⬅️ replace the old tals/... id\n",
        "qg_model     = \"valhalla/t5-small-qg-hl\"\n",
        "qa_model     = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "device       = \"auto\"\n",
        "hf_token     = None  # keep None; not required for these public repos\n",
        "from google.colab import files\n",
        "import json, os\n",
        "\n",
        "# Upload or set path\n",
        "if use_upload:\n",
        "    print(\"📂 Upload your JSON file...\")\n",
        "    uploaded = files.upload()\n",
        "    input_path = list(uploaded.keys())[0]\n",
        "else:\n",
        "    # Optional: mount Drive if needed\n",
        "    if input_path.startswith(\"/content/drive\"):\n",
        "        from google.colab import drive\n",
        "        if not os.path.ismount(\"/content/drive\"):\n",
        "            drive.mount(\"/content/drive\")\n",
        "\n",
        "print(f\"Using input: {input_path}\")\n",
        "\n",
        "# Load input\n",
        "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "assert isinstance(data, list), \"Input JSON must be a list of objects.\"\n",
        "\n",
        "# Init scorers\n",
        "print(\"🔧 Initializing models...\")\n",
        "factcc = FactCCScorer(model_name=factcc_model, device=device)\n",
        "qags   = QAGSLite(qg_model=qg_model, qa_model=qa_model, device=device)\n",
        "\n",
        "# Process\n",
        "print(\"🚀 Scoring (this can take a while on CPU)...\")\n",
        "aug = process_json(\n",
        "    records=data,\n",
        "    factcc=factcc,\n",
        "    qags=qags,\n",
        "    max_source_tokens=max_source_tokens,\n",
        "    max_summary_tokens=max_summary_tokens,\n",
        "    qags_max_questions=qags_max_questions,\n",
        ")\n",
        "\n",
        "# Save output\n",
        "base = os.path.splitext(os.path.basename(input_path))[0]\n",
        "output_path = f\"/content/{base}_scored.json\"\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(aug, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✅ Wrote: {output_path}\")\n",
        "\n",
        "# Offer download\n",
        "try:\n",
        "    files.download(output_path)\n",
        "except Exception as e:\n",
        "    print(\"You can manually download from:\", output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "uJacmpjHFkkU",
        "outputId": "a9eb591e-56c3-4fe2-f3b8-cd50223d3c20"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Upload your JSON file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3b4909c9-4db9-4ac5-acfd-b21ff081af08\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3b4909c9-4db9-4ac5-acfd-b21ff081af08\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Dialogueset_distilbart.json to Dialogueset_distilbart (2).json\n",
            "Using input: Dialogueset_distilbart (2).json\n",
            "🔧 Initializing models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Scoring (this can take a while on CPU)...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Record 0 missing keys. source in ['text','source','input'] present? True | before in ['generated_before','summary_before','generated_summary','generatedsummary_before','generatedsummary_before_EHI'] present? False | after in ['generated_after','summary_after','generated_finetuned','generatedsummary_after','generatedsummary_after_EHI'] present? False\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3114193764.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🚀 Scoring (this can take a while on CPU)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m aug = process_json(\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mfactcc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfactcc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1261997661.py\u001b[0m in \u001b[0;36mprocess_json\u001b[0;34m(records, factcc, qags, max_source_tokens, max_summary_tokens, qags_max_questions)\u001b[0m\n\u001b[1;32m    214\u001b[0m                             item.get(\"generatedsummary_after_EHI\")) # Added the new key here\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mafter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             raise KeyError(\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0;34mf\"Record {idx} missing keys. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;34mf\"source in ['text','source','input'] present? {bool(source)} | \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Record 0 missing keys. source in ['text','source','input'] present? True | before in ['generated_before','summary_before','generated_summary','generatedsummary_before','generatedsummary_before_EHI'] present? False | after in ['generated_after','summary_after','generated_finetuned','generatedsummary_after','generatedsummary_after_EHI'] present? False\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install factsumm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNxhnEcR-5ls",
        "outputId": "bcf9a8d4-52a4-4c11-b370-74532220beb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: factsumm in /usr/local/lib/python3.11/dist-packages (0.1.5)\n",
            "Requirement already satisfied: transformers==4.36.0 in /usr/local/lib/python3.11/dist-packages (from factsumm) (4.36.0)\n",
            "Requirement already satisfied: pysbd==0.3.4 in /usr/local/lib/python3.11/dist-packages (from factsumm) (0.3.4)\n",
            "Requirement already satisfied: bert-score==0.3.12 in /usr/local/lib/python3.11/dist-packages (from factsumm) (0.3.12)\n",
            "Requirement already satisfied: rich==13.7.0 in /usr/local/lib/python3.11/dist-packages (from factsumm) (13.7.0)\n",
            "Requirement already satisfied: protobuf==4.25.1 in /usr/local/lib/python3.11/dist-packages (from factsumm) (4.25.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.99 in /usr/local/lib/python3.11/dist-packages (from factsumm) (0.1.99)\n",
            "Requirement already satisfied: sumeval==0.2.2 in /usr/local/lib/python3.11/dist-packages (from factsumm) (0.2.2)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score==0.3.12->factsumm) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score==0.3.12->factsumm) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score==0.3.12->factsumm) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score==0.3.12->factsumm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score==0.3.12->factsumm) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score==0.3.12->factsumm) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score==0.3.12->factsumm) (25.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich==13.7.0->factsumm) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich==13.7.0->factsumm) (2.19.2)\n",
            "Requirement already satisfied: plac>=0.9.6 in /usr/local/lib/python3.11/dist-packages (from sumeval==0.2.2->factsumm) (1.4.5)\n",
            "Requirement already satisfied: sacrebleu>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from sumeval==0.2.2->factsumm) (2.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0->factsumm) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0->factsumm) (0.34.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0->factsumm) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0->factsumm) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0->factsumm) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0->factsumm) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0->factsumm) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0->factsumm) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0->factsumm) (1.1.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich==13.7.0->factsumm) (0.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score==0.3.12->factsumm) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score==0.3.12->factsumm) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score==0.3.12->factsumm) (2025.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.3.2->sumeval==0.2.2->factsumm) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.3.2->sumeval==0.2.2->factsumm) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.3.2->sumeval==0.2.2->factsumm) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.3.2->sumeval==0.2.2->factsumm) (5.4.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score==0.3.12->factsumm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score==0.3.12->factsumm) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score==0.3.12->factsumm) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score==0.3.12->factsumm) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score==0.3.12->factsumm) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score==0.3.12->factsumm) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score==0.3.12->factsumm) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score==0.3.12->factsumm) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score==0.3.12->factsumm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score==0.3.12->factsumm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score==0.3.12->factsumm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score==0.3.12->factsumm) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score==0.3.12->factsumm) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score==0.3.12->factsumm) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "a02ebf46",
        "outputId": "69d921a3-b354-4134-fea4-e5b0b17a4ed9"
      },
      "source": [
        "python compute_factcc_qags_scores.py --input_path input.json --output_path output.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-238069336.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-238069336.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python compute_factcc_qags_scores.py --input_path input.json --output_path output.json\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}
