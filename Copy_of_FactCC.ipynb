# -*- coding: utf-8 -*-
"""Copy of FactCC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v5PpdNIU7NnSiTdl7UV5-VMC4DjWAhU3
"""

!pip install -U transformers

"""## Local Inference on GPU
Model page: https://huggingface.co/manueldeprada/FactCC

‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/manueldeprada/FactCC)
			and/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè
"""

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-classification", model="manueldeprada/FactCC")

# Load model directly
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("manueldeprada/FactCC")
model = AutoModelForSequenceClassification.from_pretrained("manueldeprada/FactCC")

"""## Remote Inference via Inference Providers
Ensure you have a valid **HF_TOKEN** set in your environment. You can get your token from [your settings page](https://huggingface.co/settings/tokens). Note: running this may incur charges above the free tier.
The following Python example shows how to run the model remotely on HF Inference Providers, automatically selecting an available inference provider for you.
For more information on how to use the Inference Providers, please refer to our [documentation and guides](https://huggingface.co/docs/inference-providers/en/index).
"""

import os
os.environ['HF_TOKEN'] = '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'

import os
from huggingface_hub import InferenceClient

client = InferenceClient(
    provider="auto",
    api_key=os.environ["HF_TOKEN"],
)

result = client.text_classification(
    "I like you. I love you",
    model="manueldeprada/FactCC",
)

result
