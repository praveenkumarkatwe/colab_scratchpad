{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rouge_metrics_title"
      },
      "source": [
        "# Rouge Metrics Comparison Table Generator\n",
        "\n",
        "This notebook generates a comparison table of Rouge-1, Rouge-2, and Rouge-L metrics for three models (FlanT5, Mistral, DistilBART) before and after fine-tuning on the XSUM dataset.\n",
        "\n",
        "## Instructions:\n",
        "1. Upload the three JSON files from `Newset/XSUM/` directory to your Colab environment\n",
        "2. Run all cells below\n",
        "3. The table will be generated automatically\n",
        "\n",
        "## Expected Files:\n",
        "- `before_after_all_metrics_200XSUM_flant5.json` (FlanT5 data)\n",
        "- `rescore_mistral_XSUM_before_after_JSON_file.json` (Mistral data)\n",
        "- `results_DISTILBART_XSUM_Dataset_200_rescore.json` (DistilBART data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install pandas numpy --quiet\n",
        "\n",
        "print(\"‚úÖ Required packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_libraries"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "print(\"üìö Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rouge_computation_functions"
      },
      "outputs": [],
      "source": [
        "def compute_rouge_n(reference: str, candidate: str, n: int = 1) -> float:\n",
        "    \"\"\"\n",
        "    Compute ROUGE-N score between reference and candidate text.\n",
        "    \n",
        "    Args:\n",
        "        reference: Reference/target text\n",
        "        candidate: Generated/candidate text\n",
        "        n: N-gram size (1 for ROUGE-1, 2 for ROUGE-2)\n",
        "    \n",
        "    Returns:\n",
        "        ROUGE-N F1 score\n",
        "    \"\"\"\n",
        "    if not reference or not candidate:\n",
        "        return 0.0\n",
        "    \n",
        "    # Tokenize and convert to lowercase\n",
        "    ref_tokens = reference.lower().split()\n",
        "    cand_tokens = candidate.lower().split()\n",
        "    \n",
        "    if len(ref_tokens) < n or len(cand_tokens) < n:\n",
        "        return 0.0\n",
        "    \n",
        "    # Generate n-grams\n",
        "    ref_ngrams = [tuple(ref_tokens[i:i+n]) for i in range(len(ref_tokens) - n + 1)]\n",
        "    cand_ngrams = [tuple(cand_tokens[i:i+n]) for i in range(len(cand_tokens) - n + 1)]\n",
        "    \n",
        "    if not ref_ngrams or not cand_ngrams:\n",
        "        return 0.0\n",
        "    \n",
        "    # Count n-grams\n",
        "    ref_counter = Counter(ref_ngrams)\n",
        "    cand_counter = Counter(cand_ngrams)\n",
        "    \n",
        "    # Calculate overlap\n",
        "    overlap = sum((ref_counter & cand_counter).values())\n",
        "    \n",
        "    # Calculate precision and recall\n",
        "    precision = overlap / len(cand_ngrams) if len(cand_ngrams) > 0 else 0\n",
        "    recall = overlap / len(ref_ngrams) if len(ref_ngrams) > 0 else 0\n",
        "    \n",
        "    # Calculate F1 score\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def compute_rouge_l(reference: str, candidate: str) -> float:\n",
        "    \"\"\"\n",
        "    Compute ROUGE-L score using Longest Common Subsequence.\n",
        "    \n",
        "    Args:\n",
        "        reference: Reference/target text\n",
        "        candidate: Generated/candidate text\n",
        "    \n",
        "    Returns:\n",
        "        ROUGE-L F1 score\n",
        "    \"\"\"\n",
        "    if not reference or not candidate:\n",
        "        return 0.0\n",
        "    \n",
        "    ref_tokens = reference.lower().split()\n",
        "    cand_tokens = candidate.lower().split()\n",
        "    \n",
        "    if not ref_tokens or not cand_tokens:\n",
        "        return 0.0\n",
        "    \n",
        "    # Compute LCS length using dynamic programming\n",
        "    def lcs_length(seq1, seq2):\n",
        "        m, n = len(seq1), len(seq2)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "        \n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if seq1[i-1] == seq2[j-1]:\n",
        "                    dp[i][j] = dp[i-1][j-1] + 1\n",
        "                else:\n",
        "                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "        \n",
        "        return dp[m][n]\n",
        "    \n",
        "    lcs_len = lcs_length(ref_tokens, cand_tokens)\n",
        "    \n",
        "    if lcs_len == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # Calculate precision and recall\n",
        "    precision = lcs_len / len(cand_tokens)\n",
        "    recall = lcs_len / len(ref_tokens)\n",
        "    \n",
        "    # Calculate F1 score\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "print(\"üîß Rouge computation functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_processing_functions"
      },
      "outputs": [],
      "source": [
        "def extract_flant5_rouge_metrics(data: List[Dict]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Extract Rouge metrics from FlanT5 data which already contains computed Rouge scores.\n",
        "    \"\"\"\n",
        "    rouge1_before = []\n",
        "    rouge1_after = []\n",
        "    rouge2_before = []\n",
        "    rouge2_after = []\n",
        "    rougel_before = []\n",
        "    rougel_after = []\n",
        "    \n",
        "    for record in data:\n",
        "        rouge1_before.append(record.get('ROUGE1_before', 0.0))\n",
        "        rouge1_after.append(record.get('ROUGE1_after', 0.0))\n",
        "        rouge2_before.append(record.get('ROUGE2_before', 0.0))\n",
        "        rouge2_after.append(record.get('ROUGE2_after', 0.0))\n",
        "        rougel_before.append(record.get('ROUGEL_before', 0.0))\n",
        "        rougel_after.append(record.get('ROUGEL_after', 0.0))\n",
        "    \n",
        "    return {\n",
        "        'rouge1_before': np.mean(rouge1_before),\n",
        "        'rouge1_after': np.mean(rouge1_after),\n",
        "        'rouge2_before': np.mean(rouge2_before),\n",
        "        'rouge2_after': np.mean(rouge2_after),\n",
        "        'rougel_before': np.mean(rougel_before),\n",
        "        'rougel_after': np.mean(rougel_after)\n",
        "    }\n",
        "\n",
        "def compute_rouge_metrics_from_text(data: List[Dict]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute Rouge metrics from text data for models that don't have precomputed Rouge scores.\n",
        "    \"\"\"\n",
        "    rouge1_before = []\n",
        "    rouge1_after = []\n",
        "    rouge2_before = []\n",
        "    rouge2_after = []\n",
        "    rougel_before = []\n",
        "    rougel_after = []\n",
        "    \n",
        "    for record in data:\n",
        "        # Get reference and generated summaries\n",
        "        reference = record.get('reference_summary', record.get('reference', ''))\n",
        "        gen_before = record.get('gen_before', record.get('generatedsummary_before', ''))\n",
        "        gen_after = record.get('gen_after', record.get('generatedsummary_after', ''))\n",
        "        \n",
        "        if reference and gen_before:\n",
        "            rouge1_before.append(compute_rouge_n(reference, gen_before, 1))\n",
        "            rouge2_before.append(compute_rouge_n(reference, gen_before, 2))\n",
        "            rougel_before.append(compute_rouge_l(reference, gen_before))\n",
        "        \n",
        "        if reference and gen_after:\n",
        "            rouge1_after.append(compute_rouge_n(reference, gen_after, 1))\n",
        "            rouge2_after.append(compute_rouge_n(reference, gen_after, 2))\n",
        "            rougel_after.append(compute_rouge_l(reference, gen_after))\n",
        "    \n",
        "    return {\n",
        "        'rouge1_before': np.mean(rouge1_before) if rouge1_before else 0.0,\n",
        "        'rouge1_after': np.mean(rouge1_after) if rouge1_after else 0.0,\n",
        "        'rouge2_before': np.mean(rouge2_before) if rouge2_before else 0.0,\n",
        "        'rouge2_after': np.mean(rouge2_after) if rouge2_after else 0.0,\n",
        "        'rougel_before': np.mean(rougel_before) if rougel_before else 0.0,\n",
        "        'rougel_after': np.mean(rougel_after) if rougel_after else 0.0\n",
        "    }\n",
        "\n",
        "print(\"üõ†Ô∏è Data processing functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "file_paths_setup"
      },
      "outputs": [],
      "source": [
        "# Define file paths for the three JSON files\n",
        "file_paths = {\n",
        "    'FlanT5': 'before_after_all_metrics_200XSUM_flant5.json',\n",
        "    'Mistral': 'rescore_mistral_XSUM_before_after_JSON_file.json',\n",
        "    'DistilBART': 'results_DISTILBART_XSUM_Dataset_200_rescore.json'\n",
        "}\n",
        "\n",
        "# Check which files exist\n",
        "print(\"üìÅ Checking for JSON files...\")\n",
        "for model, filename in file_paths.items():\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"‚úÖ Found {model} data: {filename}\")\n",
        "    else:\n",
        "        print(f\"‚ùå Missing {model} data: {filename}\")\n",
        "        print(f\"   Please upload this file to your Colab environment\")\n",
        "\n",
        "print(\"\\nüí° If files are missing, please upload them using the Files panel on the left.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "process_data_and_generate_table"
      },
      "outputs": [],
      "source": [
        "# Process each file and compute Rouge metrics\n",
        "print(\"üîÑ Processing JSON files and computing Rouge metrics...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, file_path in file_paths.items():\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"‚ö†Ô∏è Skipping {model_name} - file not found: {file_path}\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\nüìä Processing {model_name} data...\")\n",
        "    \n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        print(f\"   ‚úÖ Loaded {len(data)} records\")\n",
        "        \n",
        "        # Check if this is FlanT5 with precomputed Rouge metrics\n",
        "        if model_name.lower() == 'flant5' and 'ROUGE1_before' in data[0]:\n",
        "            print(f\"   üéØ Using precomputed Rouge metrics\")\n",
        "            metrics = extract_flant5_rouge_metrics(data)\n",
        "        else:\n",
        "            print(f\"   üßÆ Computing Rouge metrics from text\")\n",
        "            metrics = compute_rouge_metrics_from_text(data)\n",
        "        \n",
        "        results[model_name] = metrics\n",
        "        print(f\"   ‚úÖ Completed processing {model_name}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error processing {model_name}: {e}\")\n",
        "        results[model_name] = {\n",
        "            'rouge1_before': 0.0, 'rouge1_after': 0.0,\n",
        "            'rouge2_before': 0.0, 'rouge2_after': 0.0,\n",
        "            'rougel_before': 0.0, 'rougel_after': 0.0\n",
        "        }\n",
        "\n",
        "print(\"\\n‚úÖ Data processing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_results_table"
      },
      "outputs": [],
      "source": [
        "# Create the results table\n",
        "print(\"üìã Creating Rouge Metrics Comparison Table...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Prepare data for DataFrame\n",
        "df_data = []\n",
        "for model_name, metrics in results.items():\n",
        "    df_data.append({\n",
        "        'Model': model_name,\n",
        "        'Rouge-1 Before': f\"{metrics['rouge1_before']:.4f}\",\n",
        "        'Rouge-1 After': f\"{metrics['rouge1_after']:.4f}\",\n",
        "        'Rouge-2 Before': f\"{metrics['rouge2_before']:.4f}\",\n",
        "        'Rouge-2 After': f\"{metrics['rouge2_after']:.4f}\",\n",
        "        'RougeL Before': f\"{metrics['rougel_before']:.4f}\",\n",
        "        'RougeL After': f\"{metrics['rougel_after']:.4f}\"\n",
        "    })\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(df_data)\n",
        "\n",
        "# Display the table\n",
        "print(\"üìä ROUGE METRICS COMPARISON TABLE\")\n",
        "print(\"=\" * 80)\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "print(\"\\nüìã Formatted Table (Markdown Style):\")\n",
        "print(\"-\" * 90)\n",
        "print(\"| Model     | Rouge-1 Before | Rouge-1 After | Rouge-2 Before | Rouge-2 After | RougeL Before | RougeL After |\")\n",
        "print(\"|-----------|----------------|---------------|----------------|---------------|---------------|--------------|\")\n",
        "for _, row in df.iterrows():\n",
        "    print(f\"| {row['Model']:<9} | {row['Rouge-1 Before']:<14} | {row['Rouge-1 After']:<13} | {row['Rouge-2 Before']:<14} | {row['Rouge-2 After']:<13} | {row['RougeL Before']:<13} | {row['RougeL After']:<12} |\")\n",
        "\n",
        "# Save results\n",
        "csv_filename = 'rouge_metrics_comparison.csv'\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(f\"\\nüíæ Results saved to: {csv_filename}\")\n",
        "\n",
        "print(\"\\nüéâ Analysis Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "analysis_insights"
      },
      "outputs": [],
      "source": [
        "# Provide some insights about the results\n",
        "print(\"üîç Analysis Insights:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"\\nüìà {model_name}:\")\n",
        "    \n",
        "    # Calculate changes\n",
        "    rouge1_change = metrics['rouge1_after'] - metrics['rouge1_before']\n",
        "    rouge2_change = metrics['rouge2_after'] - metrics['rouge2_before']\n",
        "    rougel_change = metrics['rougel_after'] - metrics['rougel_before']\n",
        "    \n",
        "    def format_change(change):\n",
        "        if change > 0:\n",
        "            return f\"+{change:.4f} (improved)\"\n",
        "        elif change < 0:\n",
        "            return f\"{change:.4f} (decreased)\"\n",
        "        else:\n",
        "            return \"0.0000 (no change)\"\n",
        "    \n",
        "    print(f\"   Rouge-1 Change: {format_change(rouge1_change)}\")\n",
        "    print(f\"   Rouge-2 Change: {format_change(rouge2_change)}\")\n",
        "    print(f\"   Rouge-L Change: {format_change(rougel_change)}\")\n",
        "\n",
        "print(\"\\nüí° Notes:\")\n",
        "print(\"   - Higher Rouge scores indicate better overlap with reference summaries\")\n",
        "print(\"   - Rouge-1: Unigram overlap (individual word matches)\")\n",
        "print(\"   - Rouge-2: Bigram overlap (two consecutive word matches)\")\n",
        "print(\"   - Rouge-L: Longest common subsequence overlap\")\n",
        "print(\"   - Positive changes indicate improvement after fine-tuning\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}